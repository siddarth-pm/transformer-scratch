{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "KG91tNonXEod"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleAttentionHead(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super(SingleAttentionHead, self).__init__() # init\n",
        "        self.dim = dim # dimension of model\n",
        "        self.q = nn.Linear(dim, dim) # query weights matrix\n",
        "        self.k = nn.Linear(dim, dim) # keys weights matrix\n",
        "        self.v = nn.Linear(dim, dim) # values weight matrix\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v):\n",
        "        # transpose (-2, 1)\n",
        "        attention = q.bmm(k.transpose(1, 2)) / math.sqrt(q.size(-1)) # matrix multiplying Q and K, scaling it\n",
        "        softmax = torch.softmax(attention, dim=-1) # softmax in order to normalize and make into probabilities\n",
        "        output = softmax.bmm(v) # multiply values to attention scores to get final outputs\n",
        "\n",
        "        return output\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        attentions = self.scaled_dot_product_attention(q, k, v)\n",
        "        return attentions"
      ],
      "metadata": {
        "id": "zd8gOdOmXIXU"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "  def __init__(self, dim, num_heads):\n",
        "      super(MultiHeadedAttention, self).__init__()\n",
        "      assert dim % num_heads == 0, \"model dimension must be divisible by number of heads\"\n",
        "\n",
        "      self.num_heads = num_heads # number of heads\n",
        "      self.dim_head = dim // num_heads # dim_model/num_heads, this is the dimension of each head\n",
        "\n",
        "      self.single_heads = nn.ModuleList([SingleAttentionHead(dim).to(device) for i in range(num_heads)])\n",
        "      self.linear = nn.Linear(num_heads * dim, dim)\n",
        "\n",
        "  def forward(self, q, k, v):\n",
        "    individual_head_results = [head(q, k, v) for head in self.single_heads] # Get individual head results\n",
        "    out = torch.cat(individual_head_results, dim=-1) # concatenate individual head results\n",
        "    return self.linear(out)"
      ],
      "metadata": {
        "id": "Hde1K6s-JXyY"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, dim_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(dim, dim_ff)\n",
        "        self.fc2 = nn.Linear(dim_ff, dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x))) # Densely connected layer"
      ],
      "metadata": {
        "id": "BllM-uijoo21"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "2UvEl_Qg5rw4"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implements exactly positional encoding formula from \"Attention is all you need\" paper\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_seq_length, d_model).to(device)\n",
        "        for pos in range(max_seq_length):\n",
        "          for i in range((lambda x : x // 2 + 1 if x % 2 == 1 else x // 2)(d_model)):\n",
        "            pe[pos, 2*i] = math.sin((pos/(pow(10000, 2*i / d_model))))\n",
        "            pe[pos, 2*i + 1] = math.cos((pos/pow(10000, 2*i / d_model)))\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n"
      ],
      "metadata": {
        "id": "hsuVfW47pSzi"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, dim, num_heads, dim_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadedAttention(dim, num_heads).to(device) # Attention layer\n",
        "        self.feed_forward = FeedForward(dim, dim_ff).to(device) # FF(Dense) layer\n",
        "        self.norm1 = nn.LayerNorm(dim) # Normalize\n",
        "        self.norm2 = nn.LayerNorm(dim) # Normalize\n",
        "        self.dropout = nn.Dropout(dropout) # Dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output = self.self_attn(x, x, x) # First get attention outputs\n",
        "        x = self.norm1(x + self.dropout(attn_output)) # First add and norm\n",
        "        ff_output = self.feed_forward(x) # Feed forward\n",
        "        x = self.norm2(x + self.dropout(ff_output)) # Second add and norm\n",
        "        return x"
      ],
      "metadata": {
        "id": "xRwmMOsPpVe7"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, dim, num_heads, dim_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadedAttention(dim, num_heads).to(device) # First attention, just like the others\n",
        "        self.cross_attn = MultiHeadedAttention(dim, num_heads).to(device)  # Cross attention, only difference is keys and queries come from encoder output\n",
        "        self.feed_forward = FeedForward(dim, dim_ff).to(device) # FF\n",
        "        self.norm1 = nn.LayerNorm(dim) # Norm1\n",
        "        self.norm2 = nn.LayerNorm(dim) # Norm2\n",
        "        self.norm3 = nn.LayerNorm(dim) # Norm3\n",
        "        self.dropout = nn.Dropout(dropout) # Dropout\n",
        "\n",
        "    def forward(self, x, enc_output):\n",
        "        attn_output = self.self_attn(x, x, x) # Simply uses the current sequence to generate attention output\n",
        "        x = self.norm1(x + self.dropout(attn_output)) # Add + Norm\n",
        "        # x.size() = 99, 512\n",
        "        # enc_output.size() = 100, 512\n",
        "        attn_output = self.cross_attn(x, enc_output, enc_output) # Now, uses keys and queries from encoder and values from decoder\n",
        "        x = self.norm2(x + self.dropout(attn_output)) # Add + Norm\n",
        "        ff_output = self.feed_forward(x) # FF\n",
        "        x = self.norm3(x + self.dropout(ff_output)) # Add + Norm\n",
        "        return x"
      ],
      "metadata": {
        "id": "aM22NAufpZzG"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, dim, num_heads, num_layers, dim_ff, max_seq_length, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, dim) # Encoder embedding ()\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, dim) # Decoder embedding\n",
        "        self.positional_encoding = PositionalEncoding(dim, max_seq_length).to(device) # Positional encoding\n",
        "\n",
        "        self.encoder_layers = [EncoderLayer(dim, num_heads, dim_ff, dropout).to(device) for i in range(num_layers)]\n",
        "        self.decoder_layers = [DecoderLayer(dim, num_heads, dim_ff, dropout).to(device) for i in range(num_layers)]\n",
        "\n",
        "\n",
        "        self.fc = nn.Linear(dim, tgt_vocab_size) # FC\n",
        "        self.dropout = nn.Dropout(dropout) # Dropout\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src))) # Encoder EMBEDDING-->Positional Encoding-->Dropout\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt))) # Decoder EMBEDDING-->Positional Encoding-->Dropout\n",
        "\n",
        "\n",
        "        enc_output = src_embedded # Because this is simply the encoder output (or will be)\n",
        "         # This is where we use the mask, just run through enc layers\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output)\n",
        "\n",
        "        dec_output = tgt_embedded # Decoder output (will be)\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output) # Pass in both decoder and encoder output as well as src\n",
        "\n",
        "        output = self.fc(dec_output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "d9DyjoV2pbQ6"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp_vocab_size = 5000\n",
        "target_vocab_size = 5000\n",
        "dim = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "dim_ff = 1024\n",
        "max_len = 150\n",
        "dropout = 0.1\n",
        "\n",
        "transformer = Transformer(inp_vocab_size, target_vocab_size, dim, num_heads, num_layers, dim_ff, inp_vocab_size, dropout)\n",
        "\n",
        "# Random data\n",
        "inp_dat = torch.randint(1, inp_vocab_size, (64, max_len)).to(device)  # (batch_size, seq_length)\n",
        "targets = torch.randint(1, inp_vocab_size, (64, max_len)).to(device)  # (batch_size, seq_length)"
      ],
      "metadata": {
        "id": "8emCLaqypdp5"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "transformer.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "transformer.train()\n",
        "\n",
        "num_epochs = 500\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    optimizer.zero_grad()\n",
        "    output = transformer(inp_dat, targets[:, :-1])\n",
        "    loss = criterion(output.contiguous().view(-1, target_vocab_size), targets[:, 1:].contiguous().view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 50 == 0:\n",
        "      print(\"\\nLoss: \" + str(loss.item()) + \" Epoch: \" + str(epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dsQT91jpkQK",
        "outputId": "4327e2f5-c599-47b4-c755-0791645bdad7"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/500 [00:00<03:54,  2.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loss: 8.684022903442383 Epoch: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 51/500 [00:22<03:20,  2.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loss: 7.7932963371276855 Epoch: 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 101/500 [00:45<02:59,  2.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loss: 7.108153343200684 Epoch: 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 151/500 [01:07<02:37,  2.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loss: 6.446406841278076 Epoch: 150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 201/500 [01:30<02:14,  2.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loss: 5.812021732330322 Epoch: 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 251/500 [01:52<01:51,  2.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loss: 5.187049388885498 Epoch: 250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 301/500 [02:15<01:29,  2.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loss: 4.577675819396973 Epoch: 300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 351/500 [02:37<01:06,  2.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loss: 3.9871010780334473 Epoch: 350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 401/500 [03:00<00:44,  2.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loss: 3.430532693862915 Epoch: 400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 451/500 [03:22<00:22,  2.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loss: 2.931042194366455 Epoch: 450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [03:44<00:00,  2.22it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-lm-P9UDlsj9"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}